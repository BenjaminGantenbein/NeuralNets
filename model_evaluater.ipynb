{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(test_inputs, test_labels, batch_size=50):\n",
    "    \"\"\"Convert test set to torch.Tensors and load them to DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    test_inputs, test_labels = torch.tensor(test_inputs), torch.tensor(test_labels)\n",
    "\n",
    "    # Create DataLoader for test data\n",
    "    test_data = TensorDataset(test_inputs, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_inputs, test_labels, loss_fn):\n",
    "    \n",
    "    test_dataloader = data_loader(test_inputs, test_labels)\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute predictions and accuracy\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "            total_correct += torch.sum(preds == b_labels).item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    accuracy = total_correct / len(test_dataloader.dataset)\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = b_labels.cpu().numpy()\n",
    "    confusion_mat = confusion_matrix(labels, preds)\n",
    "    report = classification_report(labels, preds)\n",
    "    f1_score_val = f1_score(labels, preds, average='macro')\n",
    "    precision_val = precision_score(labels, preds, average='macro')\n",
    "    recall_val = recall_score(labels, preds, average='macro')\n",
    "    auc_val = roc_auc_score(labels, preds, multi_class='ovo')\n",
    "    \n",
    "    print(f\"Average test loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision_val:.4f}, Recall: {recall_val:.4f}, F1-score: {f1_score_val:.4f}, AUC-ROC: {auc_val:.4f}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_mat}\")\n",
    "    print(f\"Classification Report: \\n{report}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
